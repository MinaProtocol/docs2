---
title: Mainnet database maintenance
sidebar_label: Mainnet database maintenance
hide_title: true
description: Steps to properly maintain correctness of mainnet archives
keywords:
  - Berkeley
  - upgrade
  - archive migration
  - planning 
  - prerequisites
  - mina archive node
  - archive node
  - mainnet
  - database
---

# Mainnet database in context of Berkeley Upgrade

After the berkeley migration, orginal mainnet database is not needed anymore unless you are interested in 
preserving some aspect of mainnet database which will be lost during the migration process.

After the successful migration you will have two databases:

- orginal mainnet database - with small data adjustments ( all pending blocks from last canoncial block till fork block will be converted to canoncial )
- new berkeley database based on mainnet data but:
  - without mainnet orphaned blocks
  - without mainnet pending blocks which are not a part of canoncial chain
  - with all pending blocks on canoncial chain converted to canoncial block

O1Labs and Mina Foundation teams have always made every effort to ensure that the software they produce are thoroughly tested and of the highest quality. 
However, we are only humans and we make mistakes. Such mistake occurred during work on the Mina version that was used on the mainnet for some time. 
We fixed the errors quickly. However, in decentralized world  archive nodes are not very forgiving and may contain historical issues. We are talking exclusively about 2 issues:

- Missing / Invalid Nonces - a historical issue which skewed nonces in balances table. Issue was resolved but you may still have nonces that are missing or invalid
- Incorrect ledger hashes - a historical issue which had the same root cause as 'Missing / Invalid Nonces' one. However, the outcome is that a 'replayer run' (which a operation of validating archive node against deamon ledger) is showing ledger mismatches and cannot pass problematic blocks unless ran with flag `--continue-on-error` (which can hide other issues) 

On top of that, we never forget about one problem that occurs constantly and is the nightmare of all archive node operators

- Missing blocks - everlasting issue which is cause by a disturbance in deamon node operations which can cause that archive node won't receive all blocks from network thath is supposed to hold. As a result chain continuity in archive database can be broken.

To address all above issues we released special maintenance package which fixes.

Below we describe how to install our maintenance package and fix all above issues in details:

## Installation

We provide three support for codenames:

- bullseye,
- buster,
- focal.

Below we will describe only bullseye package instalation

### Debian packages 

```
CODENAME=bullseye
CHANNEL=stable
VERSION=1.4.1

echo "deb [trusted=yes] http://packages.o1test.net $CODENAME $CHANNEL" | tee /etc/apt/sources.list.d/mina.list
apt-get update
apt-get install --allow-downgrades -y "mina-archive-maintenance=$VERSION"
```

### Docker Image

To get the Docker image:


```
docker pull gcr.io/o1labs-192920/mina-archive-maintenance:1.4.1
```

## Usage

## Missing / Invalid Nonces 

To verify Mainnet archive data, the replayer application was developed. You must run the replayer application against your existing Mainnet database to verify the blockchain state.

To run replayer:

```sh
mina-replayer \
   --archive-uri {db_connection_string} \
   --input-file reference_replayer_input.json \
   --output-file replayer_input_file.json \
   --checkpoint-interval 10000 \
   --fix-nonces \
   --set-nonces \
   --dump-repair-script

```

where:
 - `archive-uri` - connection string to archive database 
 - `output-file` - file which will hold ledger with auxiliary information like global slot and blockchain height which will be dumped on last block
 - `checkpoint-interval` - frequency of checkpoints file expressed in blocks count
 - `replayer_input_file.json` is a file constructed our of mainnet genesis ledger:
  
   ```
      jq '.ledger.accounts' genesis_ledger.json | jq  '{genesis_ledger: {accounts: .}}' > replayer_input_config.json
   ```
 - `--fix-nonces` - adjust nonces values while replaying transactions
 - `--set-nonces` - set missing nonces while replaying transactions
 - `--dump-repair-script` - path to output sql script which will contain all updates to nonces made during replayer run

Running a replayer from scratch on a Mainnet database can take up to a couple of days. The recommended best practice is to break the replayer into smaller parts by using the checkpoint capabilities of the replayer.
On top of that replayer run can have impact on archive node as it is a very resources demanding operation. It is recommended to run replayer in isolation from network connections. Preferably on isolated environment which imports mainnet dump.
In order to satisfy such need `--dump-repair-script` can be used in order to save update sql script to file system, which then can be applied on production database


##### Bad ledger hashes

There is no ultimate fix for this issue as we don't want to meddle with historical ledger hashes.
We only make sure archive integrity can be validated even with mentioned problems.
In order to verify Mainnet archive data integrity, a replayer application was developed. It has builtin mechanism of skipping errors when `--continue-on-error` flag is enabled.
However, this mode skips all the problems with integrity while our goal is to just skip blocks with bad ledger hashes.
With new version you can freely run replayer without any special flag and it will correctly handle bad ledger hashes issue.

To run replayer:

```sh
mina-replayer --archive-uri {db_connection_string} --input-file reference_replayer_input.json --output-file reference_replayer_output.json --checkpoint-interval 10000
```

where:
 - `archive-uri` - connection string to archive datbase 
 - `output-file` - file which will hold ledger with auxiliary information like global slot and blockchain height which will be dumped on last block
 - `checkpoint-interval` - frequency of checkpoints file expressed in blocks count
 - `reference_replayer_input.json` is a file constructed our of mainnet genesis ledger:
  
   ```
      jq '.ledger.accounts' genesis_ledger.json | jq  '{genesis_ledger: {accounts: .}}' > replayer_input_config.json
   ```

:warning: Running a replayer from scratch on a Mainnet database can take up to a couple of days. The recommended best practice is to break the replayer into smaller parts by using the checkpoint capabilities of the replayer.

:warning: You must run replayer from the Mainnet version. You can run it from the Docker image at minaprotocol/mina-archive:1.4.0-c980ba8-bullseye


##### Missing blocks 

The daemon node unavailability can cause the archive node to miss some of the blocks. 

If you are uploading the missing blocks to Google Cloud, the missing blocks can be reapplied from precomputed blocks and preserve chain continuity. 

To automatically verify and patch missing blocks, use the [download_missing_blocks.sh](https://raw.githubusercontent.com/MinaProtocol/mina/2.0.0berkeley_rc1/src/app/rosetta/download-missing-blocks.sh) script. Because the `download-missing-blocks` script uses localhost as the database host, unless you modify `PG_CONN` in `download_missing_block.sh`, the script assumes that psql is running on localhost on port 5432.

1. Install the required `mina-archive-blocks` and `mina-missing-blocks-auditor` scripts that are packed in the `minaprotocol/mina-archive:1.4.0-c980ba8-bullseye` Docker image.

2. Export the `BLOCKS_BUCKET`:

   ```sh
   export BLOCKS_BUCKET="https://storage.googleapis.com/my_bucket_with_precomputed_blocks"

3. Run the `mina-missing-blocks-auditor` script from the database host:

   ```sh
   download-missing-blocks.sh mainnet {db_user} {db_password}
   ```


## What steps are required for Berkeley migration?

Berkeley migration software will detect any issues on mainnet on their own. Therefore there is no need to maintain mainnet database prior to Berkeley upgrade, apart from filling missing blocks.

## Next steps

Now, we can proceed to planning berkeley migration process- [How to perform archive migration](/berkeley-upgrade/migrating-archive-database-to-berkeley)

- [Planning Berkeley archive migration](/berkeley-upgrade/migrating-archive-database-to-berkeley)
